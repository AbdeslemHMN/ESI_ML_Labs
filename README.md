# Machine Learning Labs - 2CS S1

This repository contains practical laboratory work for the Machine Learning course at ESI (École nationale Supérieure d'Informatique), covering fundamental ML concepts and algorithms through hands-on implementations.

## Objectives

- **Understand core ML algorithms** through implementation from scratch
- **Master data preprocessing** techniques for real-world datasets
- **Compare different ML approaches** (supervised learning, classification, regression)

## Labs Overview

### Workshop 01 - NumPy Fundamentals
Matrix operations, vectorization, and array manipulation for ML applications

### Workshop 02 - Data Preparation & Model Evaluation
- Data cleaning and preprocessing
- Handling heterogeneous datasets (CSV, XML, SQLite)
- Feature encoding and normalization
- Multi-label classification with sampling strategies (SMOTE, TomekLinks)
- Performance metrics and evaluation

### Workshop 03 - Neural Networks (PyTorch)
- Building neural networks with PyTorch
- Custom layers and activation functions
- Training loops and optimization
- High-level and low-level implementations

### Lab 01 - Logistic Regression
- Binary, multiclass (OvR, OvO, MaxEnt), and multilabel classification
- Cost functions (MSE, BCE, Cross-Entropy)
- Gradient descent optimization
- Bias and normalization effects on convergence

### Lab 02 - Naïve Bayes
- Multinomial and Gaussian Naïve Bayes implementations
- Prior probability and smoothing (Laplace) effects
- Text classification and performance comparison

### Lab 03 - Decision Trees & Ensemble Methods
- ID3 (Information Gain/Entropy) and CART (Gini Impurity)
- Hyperparameter tuning (depth, min samples leaf)
- Ensemble methods: Random Forest, Bagging, AdaBoost
- Variance-bias trade-offs

### Lab 04 - SVM & Optimization
- Primal and dual formulations
- Gradient descent variants (GD, SGD, SGDA)
- Kernel methods (Linear, RBF, Polynomial, Cosine)
- SMO algorithm for optimization

### Lab 06 - Regularization & Feature Selection
- L1 (Lasso) and L2 (Ridge) regularization
- Soft-thresholding and ISTA algorithm
- Filter methods (ANOVA F-test)
- Wrapper methods (Forward/Backward Feature Selection)
- Embedded methods (L1 regularization)
- Impact on overfitting and generalization


## Key Concepts Covered

- **Supervised Learning**: Classification and regression
- **Model Optimization**: Gradient descent, learning rates, convergence
- **Regularization**: L1/L2 penalties, overfitting prevention
- **Feature Engineering**: Selection, encoding, normalization, scaling
- **Model Evaluation**: Accuracy, F1-score, precision, recall, confusion matrices
- **Ensemble Learning**: Bagging, boosting, random forests
- **Kernel Methods**: Non-linear transformations for SVM
- **Multi-label Classification**: Binary relevance, sampling strategies


---

*Course: Machine Learning | Semester: S1 | Year: 2025-2026*
